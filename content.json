{"pages":[],"posts":[{"title":"Autodesk Webinar 听后感","text":"昨天凌晨2:00起来听了一个自动桌子公司的Webinar，主题是： Power up! AECOM Digitizes Construction in Civil Infrastructure 主讲人是AECOM在中东的Digital Project Delivery的一个负责人。Webinar一开始，主讲人讲了很多关于Technological Innovation和AECOM的业务Workflow之间的关系。当然这个并不是我关心的内容，关于这个Webinar我比较关注的是AECOM这样的公司在场地管理中用到了怎样的所谓Technological Innovation. 主讲人提到的Data Capture and Analysis这部分内容是我最关心的内容。不出所料,他们公司这方面主要还是使用Drone和UAS搭载的360 Camera和LIDAR。另外管理数据主要是采用Autodesk的Recap软件。以此实现场地监控 之后介绍了一个例子：利用采集的图像数据监控场地上的材料数目 之后的内容基本上和我探究的方向没什么关系了，总结一下的话这个Webinar基本上没啥我用得着的内容，他对于这些数据获取和分析的介绍都是从一个用户的角度来出发的，并没有去深入地介绍其使用了什么算法，只是介绍了其在应用阶段给工程管理带来的好处云云。具体我所关心的内容，还是去读文献比较靠谱一些。","link":"/Autodesk-Webinar-note/"},{"title":"谷歌Colab使用心得","text":"最近做毕业论文，需要做深度学习的目标识别，但是苦于算力危机，这时候发现了Google提供的超级好用的Colab，使用上手很快，但是有一些需要注意的点，做了一点总结。 最近一直在折腾毕业论文，终于可以抽一点时间写点笔记，鉴于我的记性向来很差，这还是相当有必要的，免得以后再做的时候把现在走过的弯路全走一遍。之前做图像分类很多经验就没总结，到最后全重新踩坑了。废话少说，开始说正事。 谷歌的Colab基本上就是一个深度魔改的Jupyter Notebook，但是谷歌非常良心地提供了Python的运行时，因此我们就可以直接在云端运行任何.ipynb格式的笔记了，最为让人感动的是，谷歌还提供免费的GPU资源用，拯救了我毕业论文的算力危机。当然，要是有台正经的工作站就没这么麻烦了。 首先，Colab可以使用IPython的magic command，例如%matplotlib等，但是Colab里有一部分特有的magic command，例如指定tensorflow版本的命令%tensorflow_version 1.x，由于tensorflow的object detection API还没有适配TF2.0版本，但是现在的Colab已经是默认使用TF2.0了，因此必须要在import之前就指定TF版本，否则指定完以后需要重启运行时。 同样的，很多时候我们还需要运行一些shell command，比如需要装一些缺失的包，git clone之类，可以用![shell command]，例如!pip3 install pycocotools即可。 接下来，不可避免地我们会遇到一些需要进行I/O操作的场景。直接在Colab提供的运行时中储存重要的数据是很不推荐的行为。试想训练了好久的模型，吃着火锅儿，唱着歌，突然就被谷歌把资源收回了，哭都没处哭。因此我采用的办法是用Google Drive的API，直接mount云端硬盘，这样各种操作都比较方便，而不需要次次去和云端硬盘的API打交道。使用也很简单，按照文档授权后填token即可，美中不足的就是每次mount都要填token，不过无伤大雅。需要注意的是，mount后云端硬盘的文件夹名会叫My Drive，中间的空格在各种路径中需要转义成My\\ Drive。 另外，如果要使用GPU，需要申请有GPU的运行时，在“修改-笔记本设置”里可以进行修改。需要注意GPU资源还是比较宝贵的，出于不被限额和遵守道德的原因，在编辑、调试笔记的时候不要用占用GPU运行时，等一切就绪再申请。同样即使是无GPU的运行时，也不要长久占用。 最后，也是最重要的——多看文档。刚开始没看文档，以为GPU资源不需要自己申请，结果拿CPU跑了训练，占用时间极长不说，还被暂时限制使用了，因此看文档真的很重要。 目前就这些比较重要的体会了，如果在之后有新的需要记录的内容，还会继续更新这篇博文。","link":"/Colab-Review/"},{"title":"分割COCO Format的目标识别数据集","text":"最近需要利用coco format的数据集做目标识别的训练，个人比较熟悉tensorflow，但是tensorflow的Object Detection API接受的是tfrecord格式数据，为了分别生成train validation使用的tfrecord，我需要把coco format的数据集分割一下，在这里遇到了一些麻烦。 在网上找到了别人做好的轮子： https://github.com/ashnair1/COCO-Assistant 但是问题在于，这个操作数据集的工具居然只能merge，不能split，无奈找了GitHub上这位同学的代码，对其中一些内容做了修改，最终实现了需求。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163from pycocotools.coco import COCOfrom pycocotools import mask as cocomaskimport numpy as npimport skimage.io as ioimport matplotlib.pyplot as pltimport pylabimport randomimport osfrom pycococreatortools import pycococreatortools as ptimport jsonimport randomroot_dir = '/root'INFO = { \"description\": \"Construction Site Image Dataset\", \"url\": \"\", \"version\": \"0.2.0\", \"year\": 2020, \"contributor\": \"\",}LICENSES = [ { \"id\": 1, \"name\": \"Attribution-NonCommercial-ShareAlike License\", \"url\": \"http://creativecommons.org/licenses/by-nc-sa/2.0/\" }]CATEGORIES = [ { \"id\": 1, \"name\": \"truck\", \"supercategory\": \"class\" }, { \"id\": 2, \"name\": \"excavator\", \"supercategory\": \"class\" }, { \"id\": 3, \"name\": \"crane\", \"supercategory\": \"class\" }, { \"id\": 4, \"name\": \"other_machine\", \"supercategory\": \"class\" }, { \"id\": 5, \"name\": \"precast_concrete\", \"supercategory\": \"class\" }, { \"id\": 6, \"name\": \"steel\", \"supercategory\": \"class\" }, { \"id\": 7, \"name\": \"aggregates\", \"supercategory\": \"class\" }, { \"id\": 8, \"name\": \"timber\", \"supercategory\": \"class\" }, { \"id\": 9, \"name\": \"other_materials\", \"supercategory\": \"class\" }, { \"id\": 10, \"name\": \"personnel\", \"supercategory\": \"class\" },]coco_output_train = { \"info\": INFO, \"licenses\": LICENSES, \"categories\": CATEGORIES, \"images\": [], \"annotations\": []}coco_output_val = { \"info\": INFO, \"licenses\": LICENSES, \"categories\": CATEGORIES, \"images\": [], \"annotations\": []}coco_output_test = { \"info\": INFO, \"licenses\": LICENSES, \"categories\": CATEGORIES, \"images\": [], \"annotations\": []}annotation_root_path = '/root'annotation_name = os.path.join( annotation_root_path, \"via_export_coco_300_refine.json\")# dataset = json.load(open(annotation_name, 'r'))#for ann in dataset['annotations']:# if not 'category_id' in ann.keys():# print(ann['id']) # 改标签的时候总有遗漏，导致后面步骤出问题，这里check一下，如果没有输出数字说明没有遗漏coco = COCO(annotation_name)catogry_info = coco.loadCats(coco.getCatIds( catNms=[\"truck\", \"excavator\", \"crane\",\"other_machine\",\"precast_concrete\",\"steel\",\"aggregates\",\"timber\",\"other_materials\",\"personnel\"]))coco_output_train['categories'] = catogry_infococo_output_val['categories'] = catogry_infococo_output_test['categories'] = catogry_infoimage_ids = coco.getImgIds()[0:300]random.shuffle(image_ids) #随机取样，最后再用for image_id in image_ids[:200]: image_info = coco.loadImgs(image_id)[0] coco_output_train[\"images\"].append(image_info) # annotation_ids = coco.getAnnIds(imgIds=image_id) 这句调用的函数里面有个迭代器有问题，不用这个，自己写一个 annotation_ids = [] for anno in coco.imgToAnns[str(image_id)]: annotation_ids.append(anno['id']) for annotation_id in annotation_ids: annotation_info = coco.loadAnns(annotation_id)[0] # 防止接下来的步骤再出错，输出的文件中image_id一律不再是字符串 annotation_info['image_id'] = int(annotation_info['image_id']) coco_output_train[\"annotations\"].append(annotation_info)for image_id in image_ids[200:250]: image_info = coco.loadImgs(image_id)[0] coco_output_val[\"images\"].append(image_info) # annotation_ids = coco.getAnnIds(imgIds=image_id) 这句调用的函数里面有个迭代器有问题，不用这个，自己写一个 annotation_ids = [] for anno in coco.imgToAnns[str(image_id)]: annotation_ids.append(anno['id']) for annotation_id in annotation_ids: annotation_info = coco.loadAnns(annotation_id)[0] # 防止接下来的步骤再出错，输出的文件中image_id一律不再是字符串 annotation_info['image_id'] = int(annotation_info['image_id']) coco_output_val[\"annotations\"].append(annotation_info)for image_id in image_ids[250:]: image_info = coco.loadImgs(image_id)[0] coco_output_test[\"images\"].append(image_info) # annotation_ids = coco.getAnnIds(imgIds=image_id) 这句调用的函数里面有个迭代器有问题，不用这个，自己写一个 annotation_ids = [] for anno in coco.imgToAnns[str(image_id)]: annotation_ids.append(anno['id']) for annotation_id in annotation_ids: annotation_info = coco.loadAnns(annotation_id)[0] # 防止接下来的步骤再出错，输出的文件中image_id一律不再是字符串 annotation_info['image_id'] = int(annotation_info['image_id']) coco_output_test[\"annotations\"].append(annotation_info)with open('{}/anno_train_refine.json'.format(annotation_root_path), 'w') as output_json_file: json.dump(coco_output_train, output_json_file)with open('{}/anno_val_refine.json'.format(annotation_root_path), 'w') as output_json_file: json.dump(coco_output_val, output_json_file)with open('{}/anno_test_refine.json'.format(annotation_root_path), 'w') as output_json_file: json.dump(coco_output_test, output_json_file)","link":"/coco-dataset-split/"},{"title":"LaTeX中文句号和句点的设置","text":"最近在搞毕业论文的$\\LaTeX$模版，当中遇到一点问题，就是文章的样式里中英文的句号都是一个句点，最后发现是cls文件里xeCJK宏包的设置问题。 1\\setCJKmainfont[Mapping = fullwidth-stop,BoldFont=STHeitiSC-Medium, ItalicFont=STKaitiSC-Regular]{STSongti-SC-Regular} 问题就出在那个Mapping上，fullwidth-stop会把所有句号替换成句点。当然Mapping的选项不止这几个，还可以把句点改成句号，繁简体中文转换等，这个问题的解决也得益于一位前辈的博文，非常感谢。 相关博文链接：https://matnoble.me/tech/latex/latex-period/","link":"/latex-cjk-punct/"}],"tags":[{"name":"研究笔记","slug":"研究笔记","link":"/tags/%E7%A0%94%E7%A9%B6%E7%AC%94%E8%AE%B0/"},{"name":"LaTeX","slug":"LaTeX","link":"/tags/LaTeX/"},{"name":"学习笔记","slug":"学习笔记","link":"/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"目标识别","slug":"目标识别","link":"/tags/%E7%9B%AE%E6%A0%87%E8%AF%86%E5%88%AB/"}],"categories":[]}